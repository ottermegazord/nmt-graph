{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT seq2seq English to Cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print TensorFlow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190513\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "!pip install -q -U --user tb-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "ENGLISH_TXT_PATH = 'data/questions/english.txt'\n",
    "CYPHER_TXT_PATH = 'data/questions/cypher.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts unicode file to ascii\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence_english(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    #create space between a word and the following punctuation\n",
    "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,多{}[]():->]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to each sentence so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def preprocess_sentence_cypher(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    #create space between a word and the following punctuation\n",
    "    w = re.sub(r\"([?;!,多])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,多{}[]():->]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to each sentence so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"<start> match (c:crop {name:'schima wallichii'})-[:has]->(description:description) return description.life_form ; <end>\"\n"
     ]
    }
   ],
   "source": [
    "cypher_sentence = u\"MATCH (c:crop {name:'Schima wallichii'})-[:HAS]->(description:description) RETURN description.life_form;\"\n",
    "\n",
    "print(preprocess_sentence_cypher(cypher_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean each english and cypher sentence\n",
    "# Return word pairs in format: [ENGLISH, SPANISH]\n",
    "\n",
    "def create_dataset(ENGLISH_TXT_PATH, CYPHER_TXT_PATH):\n",
    "   \n",
    "    english = []\n",
    "    cypher = []\n",
    "    \n",
    "    with open(ENGLISH_TXT_PATH) as infile:\n",
    "        for line in infile:\n",
    "            if line:\n",
    "                processed_line = preprocess_sentence_english(line)\n",
    "                english.append(processed_line)\n",
    "            \n",
    "    with open(CYPHER_TXT_PATH) as infile:\n",
    "        for line in infile:\n",
    "            if line:\n",
    "                processed_line = preprocess_sentence_cypher(line)\n",
    "                cypher.append(processed_line)\n",
    "            \n",
    "#     with open(ENGLISH_TXT_PATH) as fe:\n",
    "#         line = fe.readline()\n",
    "#         preprocess_line = preprocess_sentence_english(line)\n",
    "        \n",
    "#         english.append(preprocess_line)\n",
    "#         while line:\n",
    "#             line = fe.readline()\n",
    "#             preprocess_line = preprocess_sentence_english(line)\n",
    "#             english.append(preprocess_line)\n",
    "            \n",
    "#     with open(CYPHER_TXT_PATH) as fc:\n",
    "#         line = fc.readline()\n",
    "#         preprocess_line = preprocess_sentence_cypher(line)\n",
    "        \n",
    "#         cypher.append(preprocess_line)\n",
    "#         while line:\n",
    "#             line = fc.readline()\n",
    "#             preprocess_line = preprocess_sentence_cypher(line)\n",
    "#             cypher.append(preprocess_line)\n",
    "#     del english[-1]\n",
    "    #cypher = cypher[:-1]\n",
    "    return cypher, english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher, english = create_dataset(ENGLISH_TXT_PATH, CYPHER_TXT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> describe the habit of madagascar almond . <end>\n",
      "<start> match (a:crop_alias {name: 'madagascar almond'})-[:is_alias_of]->(c:crop) match (c)-[:has]->(description:description) return a.name , c.name , description.habit ; <end>\n"
     ]
    }
   ],
   "source": [
    "print(english[-2])\n",
    "print(cypher[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470640\n",
      "470640\n"
     ]
    }
   ],
   "source": [
    "print(len(english))\n",
    "print(len(cypher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(ENGLISH_TXT_PATH, CYPHER_TXT_PATH):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(ENGLISH_TXT_PATH, CYPHER_TXT_PATH)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(ENGLISH_TXT_PATH, CYPHER_TXT_PATH)\n",
    "\n",
    "# Calculate maximum of length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376512, 376512, 94128, 94128)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "17 ----> give\n",
      "9 ----> me\n",
      "13 ----> a\n",
      "34 ----> comprehensive\n",
      "12 ----> description\n",
      "3 ----> about\n",
      "69 ----> common\n",
      "1046 ----> horehound\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "4 ----> <start>\n",
      "2 ----> match\n",
      "6 ----> (a:crop_alias\n",
      "7 ----> {name:\n",
      "39 ----> 'common\n",
      "1554 ----> horehound'})-[:is_alias_of]->(c:crop)\n",
      "2 ----> match\n",
      "11 ----> (c)-[:has]->(ecology)\n",
      "2 ----> match\n",
      "15 ----> (c)-[:has]->(description:description)\n",
      "2 ----> match\n",
      "13 ----> (ecology)-[:grows_in]->(optimal)\n",
      "2 ----> match\n",
      "12 ----> (ecology)-[:grows_in]->(absolute)\n",
      "2 ----> match\n",
      "19 ----> (optimal)-[:consist_of]->(soil_ph_optimal:soil_ph_optimal)\n",
      "2 ----> match\n",
      "17 ----> (absolute)-[:consist_of]->(temperature:temperature_required_optimal)\n",
      "8 ----> return\n",
      "9 ----> a.name\n",
      "1 ----> ,\n",
      "10 ----> c.name\n",
      "1 ----> ,\n",
      "20 ----> description.habit\n",
      "1 ----> ,\n",
      "18 ----> description.life_form\n",
      "1 ----> ,\n",
      "21 ----> description.physiology\n",
      "1 ----> ,\n",
      "23 ----> soil_ph_optimal.max\n",
      "1 ----> ,\n",
      "22 ----> soil_ph_optimal.min\n",
      "1 ----> ,\n",
      "14 ----> temperature.min\n",
      "1 ----> ,\n",
      "16 ----> temperature.max\n",
      "3 ----> ;\n",
      "5 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 21]), TensorShape([64, 42]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 21, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 21, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 12278)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 6.0176\n",
      "Epoch 1 Batch 100 Loss 1.1830\n",
      "Epoch 1 Batch 200 Loss 0.8816\n",
      "Epoch 1 Batch 300 Loss 0.6476\n",
      "Epoch 1 Batch 400 Loss 0.4990\n",
      "Epoch 1 Batch 500 Loss 0.4205\n",
      "Epoch 1 Batch 600 Loss 0.3549\n",
      "Epoch 1 Batch 700 Loss 0.3588\n",
      "Epoch 1 Batch 800 Loss 0.4750\n",
      "Epoch 1 Batch 900 Loss 0.3923\n",
      "Epoch 1 Batch 1000 Loss 0.3332\n",
      "Epoch 1 Batch 1100 Loss 0.3218\n",
      "Epoch 1 Batch 1200 Loss 0.3481\n",
      "Epoch 1 Batch 1300 Loss 0.3041\n",
      "Epoch 1 Batch 1400 Loss 0.3299\n",
      "Epoch 1 Batch 1500 Loss 0.2966\n",
      "Epoch 1 Batch 1600 Loss 0.3020\n",
      "Epoch 1 Batch 1700 Loss 0.3081\n",
      "Epoch 1 Batch 1800 Loss 0.3143\n",
      "Epoch 1 Batch 1900 Loss 0.3153\n",
      "Epoch 1 Batch 2000 Loss 0.3032\n",
      "Epoch 1 Batch 2100 Loss 0.3081\n",
      "Epoch 1 Batch 2200 Loss 0.2861\n",
      "Epoch 1 Batch 2300 Loss 0.2908\n",
      "Epoch 1 Batch 2400 Loss 0.2678\n",
      "Epoch 1 Batch 2500 Loss 0.2730\n",
      "Epoch 1 Batch 2600 Loss 0.2559\n",
      "Epoch 1 Batch 2700 Loss 0.2756\n",
      "Epoch 1 Batch 2800 Loss 0.2874\n",
      "Epoch 1 Batch 2900 Loss 0.2632\n",
      "Epoch 1 Batch 3000 Loss 0.2668\n",
      "Epoch 1 Batch 3100 Loss 0.2591\n",
      "Epoch 1 Batch 3200 Loss 0.2606\n",
      "Epoch 1 Batch 3300 Loss 0.2512\n",
      "Epoch 1 Batch 3400 Loss 0.2455\n",
      "Epoch 1 Batch 3500 Loss 0.2708\n",
      "Epoch 1 Batch 3600 Loss 0.2616\n",
      "Epoch 1 Batch 3700 Loss 0.2498\n",
      "Epoch 1 Batch 3800 Loss 0.2488\n",
      "Epoch 1 Batch 3900 Loss 0.2394\n",
      "Epoch 1 Batch 4000 Loss 0.2349\n",
      "Epoch 1 Batch 4100 Loss 0.2377\n",
      "Epoch 1 Batch 4200 Loss 0.2352\n",
      "Epoch 1 Batch 4300 Loss 0.2360\n",
      "Epoch 1 Batch 4400 Loss 0.2373\n",
      "Epoch 1 Batch 4500 Loss 0.2268\n",
      "Epoch 1 Batch 4600 Loss 0.2212\n",
      "Epoch 1 Batch 4700 Loss 0.2313\n",
      "Epoch 1 Batch 4800 Loss 0.2235\n",
      "Epoch 1 Batch 4900 Loss 0.2181\n",
      "Epoch 1 Batch 5000 Loss 0.2228\n",
      "Epoch 1 Batch 5100 Loss 0.2151\n",
      "Epoch 1 Batch 5200 Loss 0.2217\n",
      "Epoch 1 Batch 5300 Loss 0.2151\n",
      "Epoch 1 Batch 5400 Loss 0.2189\n",
      "Epoch 1 Batch 5500 Loss 0.2084\n",
      "Epoch 1 Batch 5600 Loss 0.2143\n",
      "Epoch 1 Batch 5700 Loss 0.2042\n",
      "Epoch 1 Batch 5800 Loss 0.2001\n",
      "Epoch 1 Loss 0.3335\n",
      "Time taken for 1 epoch 1411.6471934318542 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2003\n",
      "Epoch 2 Batch 100 Loss 0.2028\n",
      "Epoch 2 Batch 200 Loss 0.1987\n",
      "Epoch 2 Batch 300 Loss 0.1906\n",
      "Epoch 2 Batch 400 Loss 0.1996\n",
      "Epoch 2 Batch 500 Loss 0.1907\n",
      "Epoch 2 Batch 600 Loss 0.1872\n",
      "Epoch 2 Batch 700 Loss 0.1945\n",
      "Epoch 2 Batch 800 Loss 0.1856\n",
      "Epoch 2 Batch 900 Loss 0.1863\n",
      "Epoch 2 Batch 1000 Loss 0.1917\n",
      "Epoch 2 Batch 1100 Loss 0.1872\n",
      "Epoch 2 Batch 1200 Loss 0.1762\n",
      "Epoch 2 Batch 1300 Loss 0.1862\n",
      "Epoch 2 Batch 1400 Loss 0.1873\n",
      "Epoch 2 Batch 1500 Loss 0.1719\n",
      "Epoch 2 Batch 1600 Loss 0.1772\n",
      "Epoch 2 Batch 1700 Loss 0.1851\n",
      "Epoch 2 Batch 1800 Loss 0.1716\n",
      "Epoch 2 Batch 1900 Loss 0.1650\n",
      "Epoch 2 Batch 2000 Loss 0.1712\n",
      "Epoch 2 Batch 2100 Loss 0.1738\n",
      "Epoch 2 Batch 2200 Loss 0.1677\n",
      "Epoch 2 Batch 2300 Loss 0.1687\n",
      "Epoch 2 Batch 2400 Loss 0.1723\n",
      "Epoch 2 Batch 2500 Loss 0.1590\n",
      "Epoch 2 Batch 2600 Loss 0.1649\n",
      "Epoch 2 Batch 2700 Loss 0.1661\n",
      "Epoch 2 Batch 2800 Loss 0.1559\n",
      "Epoch 2 Batch 2900 Loss 0.1601\n",
      "Epoch 2 Batch 3000 Loss 0.1537\n",
      "Epoch 2 Batch 3100 Loss 0.1525\n",
      "Epoch 2 Batch 3200 Loss 0.1435\n",
      "Epoch 2 Batch 3300 Loss 0.1494\n",
      "Epoch 2 Batch 3400 Loss 0.1421\n",
      "Epoch 2 Batch 3500 Loss 0.1358\n",
      "Epoch 2 Batch 3600 Loss 0.1337\n",
      "Epoch 2 Batch 3700 Loss 0.1189\n",
      "Epoch 2 Batch 3800 Loss 0.1328\n",
      "Epoch 2 Batch 3900 Loss 0.1264\n",
      "Epoch 2 Batch 4000 Loss 0.1143\n",
      "Epoch 2 Batch 4100 Loss 0.1169\n",
      "Epoch 2 Batch 4200 Loss 0.1196\n",
      "Epoch 2 Batch 4300 Loss 0.1205\n",
      "Epoch 2 Batch 4400 Loss 0.1093\n",
      "Epoch 2 Batch 4500 Loss 0.1021\n",
      "Epoch 2 Batch 4600 Loss 0.1004\n",
      "Epoch 2 Batch 4700 Loss 0.0796\n",
      "Epoch 2 Batch 4800 Loss 0.0959\n",
      "Epoch 2 Batch 4900 Loss 0.1032\n",
      "Epoch 2 Batch 5000 Loss 0.0923\n",
      "Epoch 2 Batch 5100 Loss 0.1066\n",
      "Epoch 2 Batch 5200 Loss 0.0994\n",
      "Epoch 2 Batch 5300 Loss 0.0864\n",
      "Epoch 2 Batch 5400 Loss 0.0797\n",
      "Epoch 2 Batch 5500 Loss 0.0780\n",
      "Epoch 2 Batch 5600 Loss 0.0706\n",
      "Epoch 2 Batch 5700 Loss 0.0748\n",
      "Epoch 2 Batch 5800 Loss 0.0590\n",
      "Epoch 2 Loss 0.1458\n",
      "Time taken for 1 epoch 1362.7494316101074 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0542\n",
      "Epoch 3 Batch 100 Loss 0.0596\n",
      "Epoch 3 Batch 200 Loss 0.0585\n",
      "Epoch 3 Batch 300 Loss 0.0455\n",
      "Epoch 3 Batch 400 Loss 0.0435\n",
      "Epoch 3 Batch 500 Loss 0.0524\n",
      "Epoch 3 Batch 600 Loss 0.0435\n",
      "Epoch 3 Batch 700 Loss 0.0501\n",
      "Epoch 3 Batch 800 Loss 0.0402\n",
      "Epoch 3 Batch 900 Loss 0.0318\n",
      "Epoch 3 Batch 1000 Loss 0.0356\n",
      "Epoch 3 Batch 1100 Loss 0.0283\n",
      "Epoch 3 Batch 1200 Loss 0.0282\n",
      "Epoch 3 Batch 1300 Loss 0.0220\n",
      "Epoch 3 Batch 1400 Loss 0.0211\n",
      "Epoch 3 Batch 1500 Loss 0.0212\n",
      "Epoch 3 Batch 1600 Loss 0.0271\n",
      "Epoch 3 Batch 1700 Loss 0.0199\n",
      "Epoch 3 Batch 1800 Loss 0.0211\n",
      "Epoch 3 Batch 1900 Loss 0.0240\n",
      "Epoch 3 Batch 2000 Loss 0.0246\n",
      "Epoch 3 Batch 2100 Loss 0.0134\n",
      "Epoch 3 Batch 2200 Loss 0.0099\n",
      "Epoch 3 Batch 2300 Loss 0.0082\n",
      "Epoch 3 Batch 2400 Loss 0.0097\n",
      "Epoch 3 Batch 2500 Loss 0.0065\n",
      "Epoch 3 Batch 2600 Loss 0.0152\n",
      "Epoch 3 Batch 2700 Loss 0.0106\n",
      "Epoch 3 Batch 2800 Loss 0.0088\n",
      "Epoch 3 Batch 2900 Loss 0.0062\n",
      "Epoch 3 Batch 3000 Loss 0.0058\n",
      "Epoch 3 Batch 3100 Loss 0.0073\n",
      "Epoch 3 Batch 3200 Loss 0.0049\n",
      "Epoch 3 Batch 3300 Loss 0.0076\n",
      "Epoch 3 Batch 3400 Loss 0.0049\n",
      "Epoch 3 Batch 3500 Loss 0.0035\n",
      "Epoch 3 Batch 3600 Loss 0.0017\n",
      "Epoch 3 Batch 3700 Loss 0.0020\n",
      "Epoch 3 Batch 3800 Loss 0.0027\n",
      "Epoch 3 Batch 3900 Loss 0.0016\n",
      "Epoch 3 Batch 4000 Loss 0.0007\n",
      "Epoch 3 Batch 4100 Loss 0.0034\n",
      "Epoch 3 Batch 4200 Loss 0.0010\n",
      "Epoch 3 Batch 4300 Loss 0.0011\n",
      "Epoch 3 Batch 4400 Loss 0.0007\n",
      "Epoch 3 Batch 4500 Loss 0.0013\n",
      "Epoch 3 Batch 4600 Loss 0.0014\n",
      "Epoch 3 Batch 4700 Loss 0.0003\n",
      "Epoch 3 Batch 4800 Loss 0.0008\n",
      "Epoch 3 Batch 4900 Loss 0.0003\n",
      "Epoch 3 Batch 5000 Loss 0.0004\n",
      "Epoch 3 Batch 5100 Loss 0.0004\n",
      "Epoch 3 Batch 5200 Loss 0.0004\n",
      "Epoch 3 Batch 5300 Loss 0.0003\n",
      "Epoch 3 Batch 5400 Loss 0.0002\n",
      "Epoch 3 Batch 5500 Loss 0.0003\n",
      "Epoch 3 Batch 5600 Loss 0.0002\n",
      "Epoch 3 Batch 5700 Loss 0.0002\n",
      "Epoch 3 Batch 5800 Loss 0.0002\n",
      "Epoch 3 Loss 0.0150\n",
      "Time taken for 1 epoch 1362.0940623283386 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0001\n",
      "Epoch 4 Batch 100 Loss 0.0001\n",
      "Epoch 4 Batch 200 Loss 0.0001\n",
      "Epoch 4 Batch 300 Loss 0.0002\n",
      "Epoch 4 Batch 400 Loss 0.0001\n",
      "Epoch 4 Batch 500 Loss 0.0001\n",
      "Epoch 4 Batch 600 Loss 0.0002\n",
      "Epoch 4 Batch 700 Loss 0.0002\n",
      "Epoch 4 Batch 800 Loss 0.0002\n",
      "Epoch 4 Batch 900 Loss 0.0002\n",
      "Epoch 4 Batch 1000 Loss 0.0182\n",
      "Epoch 4 Batch 1100 Loss 0.0050\n",
      "Epoch 4 Batch 1200 Loss 0.0036\n",
      "Epoch 4 Batch 1300 Loss 0.0065\n",
      "Epoch 4 Batch 1400 Loss 0.0025\n",
      "Epoch 4 Batch 1500 Loss 0.0041\n",
      "Epoch 4 Batch 1600 Loss 0.0026\n",
      "Epoch 4 Batch 1700 Loss 0.0010\n",
      "Epoch 4 Batch 1800 Loss 0.0008\n",
      "Epoch 4 Batch 1900 Loss 0.0004\n",
      "Epoch 4 Batch 2000 Loss 0.0016\n",
      "Epoch 4 Batch 2100 Loss 0.0008\n",
      "Epoch 4 Batch 2200 Loss 0.0002\n",
      "Epoch 4 Batch 2300 Loss 0.0002\n",
      "Epoch 4 Batch 2400 Loss 0.0001\n",
      "Epoch 4 Batch 2500 Loss 0.0001\n",
      "Epoch 4 Batch 2600 Loss 0.0001\n",
      "Epoch 4 Batch 2700 Loss 0.0001\n",
      "Epoch 4 Batch 2800 Loss 0.0001\n",
      "Epoch 4 Batch 2900 Loss 0.0001\n",
      "Epoch 4 Batch 3000 Loss 0.0000\n",
      "Epoch 4 Batch 3100 Loss 0.0001\n",
      "Epoch 4 Batch 3200 Loss 0.0001\n",
      "Epoch 4 Batch 3300 Loss 0.0001\n",
      "Epoch 4 Batch 3400 Loss 0.0001\n",
      "Epoch 4 Batch 3500 Loss 0.0001\n",
      "Epoch 4 Batch 3600 Loss 0.0001\n",
      "Epoch 4 Batch 3700 Loss 0.0001\n",
      "Epoch 4 Batch 3800 Loss 0.0002\n",
      "Epoch 4 Batch 3900 Loss 0.0001\n",
      "Epoch 4 Batch 4000 Loss 0.0000\n",
      "Epoch 4 Batch 4100 Loss 0.0002\n",
      "Epoch 4 Batch 4200 Loss 0.0003\n",
      "Epoch 4 Batch 4300 Loss 0.0001\n",
      "Epoch 4 Batch 4400 Loss 0.0002\n",
      "Epoch 4 Batch 4500 Loss 0.0010\n",
      "Epoch 4 Batch 4600 Loss 0.0005\n",
      "Epoch 4 Batch 4700 Loss 0.0004\n",
      "Epoch 4 Batch 4800 Loss 0.0003\n",
      "Epoch 4 Batch 4900 Loss 0.0001\n",
      "Epoch 4 Batch 5000 Loss 0.0006\n",
      "Epoch 4 Batch 5100 Loss 0.0008\n",
      "Epoch 4 Batch 5200 Loss 0.0013\n",
      "Epoch 4 Batch 5300 Loss 0.0005\n",
      "Epoch 4 Batch 5400 Loss 0.0006\n",
      "Epoch 4 Batch 5500 Loss 0.0004\n",
      "Epoch 4 Batch 5600 Loss 0.0002\n",
      "Epoch 4 Batch 5700 Loss 0.0002\n",
      "Epoch 4 Batch 5800 Loss 0.0002\n",
      "Epoch 4 Loss 0.0011\n",
      "Time taken for 1 epoch 1357.3393511772156 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0001\n",
      "Epoch 5 Batch 100 Loss 0.0003\n",
      "Epoch 5 Batch 200 Loss 0.0003\n",
      "Epoch 5 Batch 300 Loss 0.0004\n",
      "Epoch 5 Batch 400 Loss 0.0018\n",
      "Epoch 5 Batch 500 Loss 0.0002\n",
      "Epoch 5 Batch 600 Loss 0.0002\n",
      "Epoch 5 Batch 700 Loss 0.0005\n",
      "Epoch 5 Batch 800 Loss 0.0004\n",
      "Epoch 5 Batch 900 Loss 0.0001\n",
      "Epoch 5 Batch 1000 Loss 0.0003\n",
      "Epoch 5 Batch 1100 Loss 0.0000\n",
      "Epoch 5 Batch 1200 Loss 0.0001\n",
      "Epoch 5 Batch 1300 Loss 0.0000\n",
      "Epoch 5 Batch 1400 Loss 0.0004\n",
      "Epoch 5 Batch 1500 Loss 0.0002\n",
      "Epoch 5 Batch 1600 Loss 0.0000\n",
      "Epoch 5 Batch 1700 Loss 0.0000\n",
      "Epoch 5 Batch 1800 Loss 0.0002\n",
      "Epoch 5 Batch 1900 Loss 0.0000\n",
      "Epoch 5 Batch 2000 Loss 0.0000\n",
      "Epoch 5 Batch 2100 Loss 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 2200 Loss 0.0000\n",
      "Epoch 5 Batch 2300 Loss 0.0000\n",
      "Epoch 5 Batch 2400 Loss 0.0000\n",
      "Epoch 5 Batch 2500 Loss 0.0000\n",
      "Epoch 5 Batch 2600 Loss 0.0000\n",
      "Epoch 5 Batch 2700 Loss 0.0000\n",
      "Epoch 5 Batch 2800 Loss 0.0000\n",
      "Epoch 5 Batch 2900 Loss 0.0000\n",
      "Epoch 5 Batch 3000 Loss 0.0000\n",
      "Epoch 5 Batch 3100 Loss 0.0000\n",
      "Epoch 5 Batch 3200 Loss 0.0000\n",
      "Epoch 5 Batch 3300 Loss 0.0000\n",
      "Epoch 5 Batch 3400 Loss 0.0000\n",
      "Epoch 5 Batch 3500 Loss 0.0000\n",
      "Epoch 5 Batch 3600 Loss 0.0000\n",
      "Epoch 5 Batch 3700 Loss 0.0000\n",
      "Epoch 5 Batch 3800 Loss 0.0000\n",
      "Epoch 5 Batch 3900 Loss 0.0000\n",
      "Epoch 5 Batch 4000 Loss 0.0135\n",
      "Epoch 5 Batch 4100 Loss 0.0013\n",
      "Epoch 5 Batch 4200 Loss 0.0010\n",
      "Epoch 5 Batch 4300 Loss 0.0002\n",
      "Epoch 5 Batch 4400 Loss 0.0008\n",
      "Epoch 5 Batch 4500 Loss 0.0008\n",
      "Epoch 5 Batch 4600 Loss 0.0006\n",
      "Epoch 5 Batch 4700 Loss 0.0002\n",
      "Epoch 5 Batch 4800 Loss 0.0006\n",
      "Epoch 5 Batch 4900 Loss 0.0001\n",
      "Epoch 5 Batch 5000 Loss 0.0001\n",
      "Epoch 5 Batch 5100 Loss 0.0001\n",
      "Epoch 5 Batch 5200 Loss 0.0001\n",
      "Epoch 5 Batch 5300 Loss 0.0003\n",
      "Epoch 5 Batch 5400 Loss 0.0001\n",
      "Epoch 5 Batch 5500 Loss 0.0001\n",
      "Epoch 5 Batch 5600 Loss 0.0025\n",
      "Epoch 5 Batch 5700 Loss 0.0000\n",
      "Epoch 5 Batch 5800 Loss 0.0000\n",
      "Epoch 5 Loss 0.0006\n",
      "Time taken for 1 epoch 1356.9711620807648 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0000\n",
      "Epoch 6 Batch 100 Loss 0.0000\n",
      "Epoch 6 Batch 200 Loss 0.0000\n",
      "Epoch 6 Batch 300 Loss 0.0006\n",
      "Epoch 6 Batch 400 Loss 0.0000\n",
      "Epoch 6 Batch 500 Loss 0.0000\n",
      "Epoch 6 Batch 600 Loss 0.0000\n",
      "Epoch 6 Batch 700 Loss 0.0000\n",
      "Epoch 6 Batch 800 Loss 0.0000\n",
      "Epoch 6 Batch 900 Loss 0.0000\n",
      "Epoch 6 Batch 1000 Loss 0.0000\n",
      "Epoch 6 Batch 1100 Loss 0.0000\n",
      "Epoch 6 Batch 1200 Loss 0.0000\n",
      "Epoch 6 Batch 1300 Loss 0.0000\n",
      "Epoch 6 Batch 1400 Loss 0.0000\n",
      "Epoch 6 Batch 1500 Loss 0.0000\n",
      "Epoch 6 Batch 1600 Loss 0.0000\n",
      "Epoch 6 Batch 1700 Loss 0.0000\n",
      "Epoch 6 Batch 1800 Loss 0.0000\n",
      "Epoch 6 Batch 1900 Loss 0.0000\n",
      "Epoch 6 Batch 2000 Loss 0.0000\n",
      "Epoch 6 Batch 2100 Loss 0.0000\n",
      "Epoch 6 Batch 2200 Loss 0.0000\n",
      "Epoch 6 Batch 2300 Loss 0.0009\n",
      "Epoch 6 Batch 2400 Loss 0.0000\n",
      "Epoch 6 Batch 2500 Loss 0.0000\n",
      "Epoch 6 Batch 2600 Loss 0.0000\n",
      "Epoch 6 Batch 2700 Loss 0.0000\n",
      "Epoch 6 Batch 2800 Loss 0.0000\n",
      "Epoch 6 Batch 2900 Loss 0.0000\n",
      "Epoch 6 Batch 3000 Loss 0.0000\n",
      "Epoch 6 Batch 3100 Loss 0.0000\n",
      "Epoch 6 Batch 3200 Loss 0.0000\n",
      "Epoch 6 Batch 3300 Loss 0.0007\n",
      "Epoch 6 Batch 3400 Loss 0.0033\n",
      "Epoch 6 Batch 3500 Loss 0.0011\n",
      "Epoch 6 Batch 3600 Loss 0.0003\n",
      "Epoch 6 Batch 3700 Loss 0.0014\n",
      "Epoch 6 Batch 3800 Loss 0.0009\n",
      "Epoch 6 Batch 3900 Loss 0.0005\n",
      "Epoch 6 Batch 4000 Loss 0.0000\n",
      "Epoch 6 Batch 4100 Loss 0.0001\n",
      "Epoch 6 Batch 4200 Loss 0.0000\n",
      "Epoch 6 Batch 4300 Loss 0.0000\n",
      "Epoch 6 Batch 4400 Loss 0.0000\n",
      "Epoch 6 Batch 4500 Loss 0.0000\n",
      "Epoch 6 Batch 4600 Loss 0.0000\n",
      "Epoch 6 Batch 4700 Loss 0.0000\n",
      "Epoch 6 Batch 4800 Loss 0.0000\n",
      "Epoch 6 Batch 4900 Loss 0.0000\n",
      "Epoch 6 Batch 5000 Loss 0.0000\n",
      "Epoch 6 Batch 5100 Loss 0.0000\n",
      "Epoch 6 Batch 5200 Loss 0.0000\n",
      "Epoch 6 Batch 5300 Loss 0.0000\n",
      "Epoch 6 Batch 5400 Loss 0.0000\n",
      "Epoch 6 Batch 5500 Loss 0.0000\n",
      "Epoch 6 Batch 5600 Loss 0.0000\n",
      "Epoch 6 Batch 5700 Loss 0.0000\n",
      "Epoch 6 Batch 5800 Loss 0.0000\n",
      "Epoch 6 Loss 0.0002\n",
      "Time taken for 1 epoch 1363.282471895218 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0000\n",
      "Epoch 7 Batch 100 Loss 0.0001\n",
      "Epoch 7 Batch 200 Loss 0.0001\n",
      "Epoch 7 Batch 300 Loss 0.0001\n",
      "Epoch 7 Batch 400 Loss 0.0002\n",
      "Epoch 7 Batch 500 Loss 0.0003\n",
      "Epoch 7 Batch 600 Loss 0.0003\n",
      "Epoch 7 Batch 700 Loss 0.0001\n",
      "Epoch 7 Batch 800 Loss 0.0001\n",
      "Epoch 7 Batch 900 Loss 0.0001\n",
      "Epoch 7 Batch 1000 Loss 0.0001\n",
      "Epoch 7 Batch 1100 Loss 0.0000\n",
      "Epoch 7 Batch 1200 Loss 0.0003\n",
      "Epoch 7 Batch 1300 Loss 0.0000\n",
      "Epoch 7 Batch 1400 Loss 0.0000\n",
      "Epoch 7 Batch 1500 Loss 0.0000\n",
      "Epoch 7 Batch 1600 Loss 0.0000\n",
      "Epoch 7 Batch 1700 Loss 0.0000\n",
      "Epoch 7 Batch 1800 Loss 0.0000\n",
      "Epoch 7 Batch 1900 Loss 0.0000\n",
      "Epoch 7 Batch 2000 Loss 0.0007\n",
      "Epoch 7 Batch 2100 Loss 0.0007\n",
      "Epoch 7 Batch 2200 Loss 0.0001\n",
      "Epoch 7 Batch 2300 Loss 0.0000\n",
      "Epoch 7 Batch 2400 Loss 0.0001\n",
      "Epoch 7 Batch 2500 Loss 0.0001\n",
      "Epoch 7 Batch 2600 Loss 0.0014\n",
      "Epoch 7 Batch 2700 Loss 0.0000\n",
      "Epoch 7 Batch 2800 Loss 0.0000\n",
      "Epoch 7 Batch 2900 Loss 0.0000\n",
      "Epoch 7 Batch 3000 Loss 0.0000\n",
      "Epoch 7 Batch 3100 Loss 0.0000\n",
      "Epoch 7 Batch 3200 Loss 0.0000\n",
      "Epoch 7 Batch 3300 Loss 0.0000\n",
      "Epoch 7 Batch 3400 Loss 0.0000\n",
      "Epoch 7 Batch 3500 Loss 0.0001\n",
      "Epoch 7 Batch 3600 Loss 0.0000\n",
      "Epoch 7 Batch 3700 Loss 0.0000\n",
      "Epoch 7 Batch 3800 Loss 0.0000\n",
      "Epoch 7 Batch 3900 Loss 0.0000\n",
      "Epoch 7 Batch 4000 Loss 0.0000\n",
      "Epoch 7 Batch 4100 Loss 0.0000\n",
      "Epoch 7 Batch 4200 Loss 0.0000\n",
      "Epoch 7 Batch 4300 Loss 0.0000\n",
      "Epoch 7 Batch 4400 Loss 0.0000\n",
      "Epoch 7 Batch 4500 Loss 0.0000\n",
      "Epoch 7 Batch 4600 Loss 0.0000\n",
      "Epoch 7 Batch 4700 Loss 0.0000\n",
      "Epoch 7 Batch 4800 Loss 0.0000\n",
      "Epoch 7 Batch 4900 Loss 0.0000\n",
      "Epoch 7 Batch 5000 Loss 0.0000\n",
      "Epoch 7 Batch 5100 Loss 0.0000\n",
      "Epoch 7 Batch 5200 Loss 0.0000\n",
      "Epoch 7 Batch 5300 Loss 0.0000\n",
      "Epoch 7 Batch 5400 Loss 0.0000\n",
      "Epoch 7 Batch 5500 Loss 0.0000\n",
      "Epoch 7 Batch 5600 Loss 0.0000\n",
      "Epoch 7 Batch 5700 Loss 0.0000\n",
      "Epoch 7 Batch 5800 Loss 0.0000\n",
      "Epoch 7 Loss 0.0001\n",
      "Time taken for 1 epoch 1359.4380960464478 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0000\n",
      "Epoch 8 Batch 100 Loss 0.0000\n",
      "Epoch 8 Batch 200 Loss 0.0000\n",
      "Epoch 8 Batch 300 Loss 0.0000\n",
      "Epoch 8 Batch 400 Loss 0.0000\n",
      "Epoch 8 Batch 500 Loss 0.0000\n",
      "Epoch 8 Batch 600 Loss 0.0000\n",
      "Epoch 8 Batch 700 Loss 0.0000\n",
      "Epoch 8 Batch 800 Loss 0.0000\n",
      "Epoch 8 Batch 900 Loss 0.0000\n",
      "Epoch 8 Batch 1000 Loss 0.0000\n",
      "Epoch 8 Batch 1100 Loss 0.0000\n",
      "Epoch 8 Batch 1200 Loss 0.0000\n",
      "Epoch 8 Batch 1300 Loss 0.0000\n",
      "Epoch 8 Batch 1400 Loss 0.0000\n",
      "Epoch 8 Batch 1500 Loss 0.0000\n",
      "Epoch 8 Batch 1600 Loss 0.0000\n",
      "Epoch 8 Batch 1700 Loss 0.0000\n",
      "Epoch 8 Batch 1800 Loss 0.0000\n",
      "Epoch 8 Batch 1900 Loss 0.0000\n",
      "Epoch 8 Batch 2000 Loss 0.0000\n",
      "Epoch 8 Batch 2100 Loss 0.0000\n",
      "Epoch 8 Batch 2200 Loss 0.0000\n",
      "Epoch 8 Batch 2300 Loss 0.0000\n",
      "Epoch 8 Batch 2400 Loss 0.0000\n",
      "Epoch 8 Batch 2500 Loss 0.0000\n",
      "Epoch 8 Batch 2600 Loss 0.0000\n",
      "Epoch 8 Batch 2700 Loss 0.0000\n",
      "Epoch 8 Batch 2800 Loss 0.0000\n",
      "Epoch 8 Batch 2900 Loss 0.0000\n",
      "Epoch 8 Batch 3000 Loss 0.0000\n",
      "Epoch 8 Batch 3100 Loss 0.0000\n",
      "Epoch 8 Batch 3200 Loss 0.0000\n",
      "Epoch 8 Batch 3300 Loss 0.0000\n",
      "Epoch 8 Batch 3400 Loss 0.0000\n",
      "Epoch 8 Batch 3500 Loss 0.0000\n",
      "Epoch 8 Batch 3600 Loss 0.0000\n",
      "Epoch 8 Batch 3700 Loss 0.0000\n",
      "Epoch 8 Batch 3800 Loss 0.0000\n",
      "Epoch 8 Batch 3900 Loss 0.0000\n",
      "Epoch 8 Batch 4000 Loss 0.0000\n",
      "Epoch 8 Batch 4100 Loss 0.0000\n",
      "Epoch 8 Batch 4200 Loss 0.0000\n",
      "Epoch 8 Batch 4300 Loss 0.0000\n",
      "Epoch 8 Batch 4400 Loss 0.0000\n",
      "Epoch 8 Batch 4500 Loss 0.0000\n",
      "Epoch 8 Batch 4600 Loss 0.0000\n",
      "Epoch 8 Batch 4700 Loss 0.0000\n",
      "Epoch 8 Batch 4800 Loss 0.0000\n",
      "Epoch 8 Batch 4900 Loss 0.0000\n",
      "Epoch 8 Batch 5000 Loss 0.0000\n",
      "Epoch 8 Batch 5100 Loss 0.0000\n",
      "Epoch 8 Batch 5200 Loss 0.0000\n",
      "Epoch 8 Batch 5300 Loss 0.0000\n",
      "Epoch 8 Batch 5400 Loss 0.0000\n",
      "Epoch 8 Batch 5500 Loss 0.0000\n",
      "Epoch 8 Batch 5600 Loss 0.0000\n",
      "Epoch 8 Batch 5700 Loss 0.0000\n",
      "Epoch 8 Batch 5800 Loss 0.0000\n",
      "Epoch 8 Loss 0.0000\n",
      "Time taken for 1 epoch 1359.1651306152344 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0000\n",
      "Epoch 9 Batch 100 Loss 0.0000\n",
      "Epoch 9 Batch 200 Loss 0.0000\n",
      "Epoch 9 Batch 300 Loss 0.0000\n",
      "Epoch 9 Batch 400 Loss 0.0000\n",
      "Epoch 9 Batch 500 Loss 0.0000\n",
      "Epoch 9 Batch 600 Loss 0.0000\n",
      "Epoch 9 Batch 700 Loss 0.0000\n",
      "Epoch 9 Batch 800 Loss 0.0000\n",
      "Epoch 9 Batch 900 Loss 0.0000\n",
      "Epoch 9 Batch 1000 Loss 0.0000\n",
      "Epoch 9 Batch 1100 Loss 0.0000\n",
      "Epoch 9 Batch 1200 Loss 0.0000\n",
      "Epoch 9 Batch 1300 Loss 0.0000\n",
      "Epoch 9 Batch 1400 Loss 0.0000\n",
      "Epoch 9 Batch 1500 Loss 0.0000\n",
      "Epoch 9 Batch 1600 Loss 0.0000\n",
      "Epoch 9 Batch 1700 Loss 0.0000\n",
      "Epoch 9 Batch 1800 Loss 0.0000\n",
      "Epoch 9 Batch 1900 Loss 0.0000\n",
      "Epoch 9 Batch 2000 Loss 0.0000\n",
      "Epoch 9 Batch 2100 Loss 0.0000\n",
      "Epoch 9 Batch 2200 Loss 0.0000\n",
      "Epoch 9 Batch 2300 Loss 0.0000\n",
      "Epoch 9 Batch 2400 Loss 0.0000\n",
      "Epoch 9 Batch 2500 Loss 0.0000\n",
      "Epoch 9 Batch 2600 Loss 0.0000\n",
      "Epoch 9 Batch 2700 Loss 0.0000\n",
      "Epoch 9 Batch 2800 Loss 0.0000\n",
      "Epoch 9 Batch 2900 Loss 0.0000\n",
      "Epoch 9 Batch 3000 Loss 0.0000\n",
      "Epoch 9 Batch 3100 Loss 0.0000\n",
      "Epoch 9 Batch 3200 Loss 0.0000\n",
      "Epoch 9 Batch 3300 Loss 0.0000\n",
      "Epoch 9 Batch 3400 Loss 0.0000\n",
      "Epoch 9 Batch 3500 Loss 0.0000\n",
      "Epoch 9 Batch 3600 Loss 0.0000\n",
      "Epoch 9 Batch 3700 Loss 0.0000\n",
      "Epoch 9 Batch 3800 Loss 0.0000\n",
      "Epoch 9 Batch 3900 Loss 0.0000\n",
      "Epoch 9 Batch 4000 Loss 0.0000\n",
      "Epoch 9 Batch 4100 Loss 0.0000\n",
      "Epoch 9 Batch 4200 Loss 0.0000\n",
      "Epoch 9 Batch 4300 Loss 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 4400 Loss 0.0000\n",
      "Epoch 9 Batch 4500 Loss 0.0000\n",
      "Epoch 9 Batch 4600 Loss 0.0000\n",
      "Epoch 9 Batch 4700 Loss 0.0000\n",
      "Epoch 9 Batch 4800 Loss 0.0000\n",
      "Epoch 9 Batch 4900 Loss 0.0000\n",
      "Epoch 9 Batch 5000 Loss 0.0000\n",
      "Epoch 9 Batch 5100 Loss 0.0000\n",
      "Epoch 9 Batch 5200 Loss 0.0000\n",
      "Epoch 9 Batch 5300 Loss 0.0000\n",
      "Epoch 9 Batch 5400 Loss 0.0000\n",
      "Epoch 9 Batch 5500 Loss 0.0000\n",
      "Epoch 9 Batch 5600 Loss 0.0000\n",
      "Epoch 9 Batch 5700 Loss 0.0000\n",
      "Epoch 9 Batch 5800 Loss 0.0000\n",
      "Epoch 9 Loss 0.0000\n",
      "Time taken for 1 epoch 1359.4686961174011 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0000\n",
      "Epoch 10 Batch 100 Loss 0.0000\n",
      "Epoch 10 Batch 200 Loss 0.0000\n",
      "Epoch 10 Batch 300 Loss 0.0000\n",
      "Epoch 10 Batch 400 Loss 0.0000\n",
      "Epoch 10 Batch 500 Loss 0.0000\n",
      "Epoch 10 Batch 600 Loss 0.0000\n",
      "Epoch 10 Batch 700 Loss 0.0000\n",
      "Epoch 10 Batch 800 Loss 0.0000\n",
      "Epoch 10 Batch 900 Loss 0.0313\n",
      "Epoch 10 Batch 1000 Loss 0.0035\n",
      "Epoch 10 Batch 1100 Loss 0.0009\n",
      "Epoch 10 Batch 1200 Loss 0.0019\n",
      "Epoch 10 Batch 1300 Loss 0.0002\n",
      "Epoch 10 Batch 1400 Loss 0.0000\n",
      "Epoch 10 Batch 1500 Loss 0.0001\n",
      "Epoch 10 Batch 1600 Loss 0.0001\n",
      "Epoch 10 Batch 1700 Loss 0.0000\n",
      "Epoch 10 Batch 1800 Loss 0.0000\n",
      "Epoch 10 Batch 1900 Loss 0.0000\n",
      "Epoch 10 Batch 2000 Loss 0.0000\n",
      "Epoch 10 Batch 2100 Loss 0.0001\n",
      "Epoch 10 Batch 2200 Loss 0.0000\n",
      "Epoch 10 Batch 2300 Loss 0.0000\n",
      "Epoch 10 Batch 2400 Loss 0.0000\n",
      "Epoch 10 Batch 2500 Loss 0.0000\n",
      "Epoch 10 Batch 2600 Loss 0.0000\n",
      "Epoch 10 Batch 2700 Loss 0.0000\n",
      "Epoch 10 Batch 2800 Loss 0.0004\n",
      "Epoch 10 Batch 2900 Loss 0.0000\n",
      "Epoch 10 Batch 3000 Loss 0.0000\n",
      "Epoch 10 Batch 3100 Loss 0.0000\n",
      "Epoch 10 Batch 3200 Loss 0.0000\n",
      "Epoch 10 Batch 3300 Loss 0.0030\n",
      "Epoch 10 Batch 3400 Loss 0.0000\n",
      "Epoch 10 Batch 3500 Loss 0.0000\n",
      "Epoch 10 Batch 3600 Loss 0.0000\n",
      "Epoch 10 Batch 3700 Loss 0.0000\n",
      "Epoch 10 Batch 3800 Loss 0.0000\n",
      "Epoch 10 Batch 3900 Loss 0.0000\n",
      "Epoch 10 Batch 4000 Loss 0.0000\n",
      "Epoch 10 Batch 4100 Loss 0.0000\n",
      "Epoch 10 Batch 4200 Loss 0.0000\n",
      "Epoch 10 Batch 4300 Loss 0.0016\n",
      "Epoch 10 Batch 4400 Loss 0.0005\n",
      "Epoch 10 Batch 4500 Loss 0.0002\n",
      "Epoch 10 Batch 4600 Loss 0.0001\n",
      "Epoch 10 Batch 4700 Loss 0.0001\n",
      "Epoch 10 Batch 4800 Loss 0.0001\n",
      "Epoch 10 Batch 4900 Loss 0.0000\n",
      "Epoch 10 Batch 5000 Loss 0.0000\n",
      "Epoch 10 Batch 5100 Loss 0.0000\n",
      "Epoch 10 Batch 5200 Loss 0.0000\n",
      "Epoch 10 Batch 5300 Loss 0.0001\n",
      "Epoch 10 Batch 5400 Loss 0.0000\n",
      "Epoch 10 Batch 5500 Loss 0.0000\n",
      "Epoch 10 Batch 5600 Loss 0.0000\n",
      "Epoch 10 Batch 5700 Loss 0.0000\n",
      "Epoch 10 Batch 5800 Loss 0.0003\n",
      "Epoch 10 Loss 0.0005\n",
      "Time taken for 1 epoch 1358.0887925624847 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "#         tf.summary.scalar('batch_loss', batch_loss.numpy(), step=epoch)\n",
    "\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence_english(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "graph = Graph(password=\"farmers@heart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "#     print('Input: %s' % (sentence))\n",
    "#     print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    clean_sentence = result.replace('<start>', '')\n",
    "    clean_sentence = result.replace('<end>', '')\n",
    "#     print(clean_sentence)\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    \n",
    "    print(graph.run(clean_sentence).to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f29c2675fd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a.name | c.name           \r\n",
      "--------|------------------\r\n",
      " mango  | mangifera indica \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(u'What is the scientific name of mango?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> what is the minimum temperature to grow muli ? <end>\n",
      "<start> match (a:crop_alias {name: 'muli'})-[:is_alias_of]->(c:crop) match (c)-[:has]->(ecology) match (ecology)-[:grows_in]->(absolute) match (absolute)-[:consist_of]->(temperature:temperature_required_optimal) return a.name , c.name , temperature.min ; <end>\n"
     ]
    }
   ],
   "source": [
    "print(english[1050])\n",
    "print(cypher[1050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>temperature.min</th><th>temperature.max</th></tr><tr><td style=\"text-align:right\">8.0</td><td style=\"text-align:right\">20.0</td></tr></table>"
      ],
      "text/plain": [
       " temperature.min | temperature.max \n",
       "-----------------|-----------------\n",
       "             8.0 |            20.0 "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(\"match (c:crop {name:'hippophae rhamnoides'})-[:has]->(ecology) match (ecology)-[:grows_in]->(optimal) match (optimal)-[:consist_of]->(temperature:temperature_required_optimal) return temperature.min , temperature.max ;\").to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> describe the life form of dakhar . <end>\n",
      "<start> match (a:crop_alias {name: 'dakhar'})-[:is_alias_of]->(c:crop) match (c)-[:has]->(description:description) return a.name , c.name , description.life_form ; <end>\n"
     ]
    }
   ],
   "source": [
    "print(english[15000])\n",
    "print(cypher[15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow Two",
   "language": "python",
   "name": "tensorflow-two"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
